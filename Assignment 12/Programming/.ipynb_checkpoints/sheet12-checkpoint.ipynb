{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Backpropagation (50 P)\n",
    "\n",
    "In this programming exercise, we will experiment with backpropagation networks on a simple nonlinear two-dimensional classification dataset. In particular, we will try to find an explicit feature map in which the classification problem can be solved by a linear SVM. The gradient of the SVM objective with respect to its inputs will be backpropagated into the neural network in order to update the neural network parameters. The backbone implementation of the neural network is provided in the file `modules.py`.\n",
    "\n",
    "## Backpropagating the Gradient of a SVM in a Neural Network\n",
    "\n",
    "Let $x \\to \\phi(x;\\theta)$ be a feature map implemented by a multilayer neural network, where $\\theta$ is the set of all neural network parameters.\n",
    "\n",
    "The L2 soft-margin SVM that operates on top of the neural network feature map representation learns a large-margin classification boundary that minimizes the objective given in unconstrained form as:\n",
    "$$\\qquad J = \\frac1C ||w||^2 + \\sum_{i=1}^n \\max(0,1-y_i (w^\\top \\phi(x_i;\\theta) + b))^2$$\n",
    "\n",
    "where the SVM parameter $w$ is of same dimension as the feature map and where $b$ is a bias term. From this objective, we can derive the SVM gradient with respect to the feature map of a data point $x_i$ given as input:\n",
    "$$\\qquad \\frac{\\partial J}{\\partial \\phi(x_i;\\theta)} = - 2 \\max(0,1-y_i (w^\\top \\phi(x_i) + b)) \\cdot y_i \\cdot w$$\n",
    "\n",
    "The gradient of the SVM is already implemented by `utils.LinearSVC` in the file `utils.py`. Once the SVM gradient is computed, we would like to backpropagate the gradient of the SVM into the neural network in order to update the parameters $\\theta$. This is achieved by using the chain rule and computing the gradient:\n",
    "$$\\frac{\\partial J}{\\partial \\theta} = \\sum_i \\frac{\\partial \\phi(x_i;\\theta)}{\\partial \\theta} \\cdot \\frac{\\partial J}{\\partial \\phi(x_i;\\theta)}$$\n",
    "\n",
    "## The `train()` Function\n",
    "\n",
    "The following method learns a neural network representation on the 2D toy dataset provided in `datasets.py` by backpropagating the gradient of a SVC into the neural network and performing gradient descent. Note that at each iteration, a SVC classifier is optimized in order to get the most up-to-date gradient.\n",
    "\n",
    "* At each `mstep`, the procedure prints the current value of the SVC objective when the the SVC is trained either on some training or test data. (Note that the neural network never sees the test data). The difference of SVC error on the training and test data gives a good idea of whether the representation learned by the neural network generalizes well on the test data.\n",
    "\n",
    "* At each `vstep`, the procedure displays a visualization (`utils.visualize(...)`) of what has been learned by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datasets,numpy\n",
    "\n",
    "def train(nn,svc,msteps=[],vsteps=[],lr=0.1):\n",
    "    \n",
    "    X,Y = datasets.toy()\n",
    "    Xtest,Ytest = datasets.toy(test=True)\n",
    "    \n",
    "    # Train the neural network for 100 iterations\n",
    "    for it in range(1,101):\n",
    "        \n",
    "        # Perform one step of gradient descent in the neural network\n",
    "        Z = nn.forward(X)\n",
    "        svc.optimize(Z,Y)\n",
    "        DZ = svc.gradient()\n",
    "        nn.backward(DZ)\n",
    "        nn.update(lr)\n",
    "    \n",
    "        # Print current error\n",
    "        if it in msteps:\n",
    "            print('%4d J=%7.3f Jtest=%7.3f'%(it,svc.optimize(nn.forward(X),Y),svc.optimize(nn.forward(Xtest),Ytest)))\n",
    "            \n",
    "        # Visualize what has been learned\n",
    "        if it in vsteps:\n",
    "            Ztest = nn.forward(Xtest)\n",
    "            svc.optimize(Ztest,Ytest)\n",
    "            DZtest = svc.gradient()\n",
    "            Ftest  = svc.predictions()\n",
    "            DXtest = nn.backward(DZtest)\n",
    "            utils.visualize(Xtest,DXtest,Ftest,Ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Feature Map\n",
    "\n",
    "In the code below, a particular neural network feature map is considered: a linear transformation (`modules.Linear`) followed by a linear neuron activation function (`modules.Identity`) that applies similarly to each 20 neurons. One can observe by running the code, that the 20-dimensional linear feature map does not allow for nonlinear classification. The rightmost plot shows the gradient of the objective $\\partial J / \\partial x_i$ with respect to each data point $x_1,\\dots,x_n$ in the dataset. Points outside the margin have no SVC gradient (i.e. are on the right side of the SVM margin and do not influence the classifier). On the other hand, points that are inside the margin or on the opposite side have a non-zero gradient, as shown by the arrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (modules.py, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"E:\\Academic\\ML\\Assignments\\Assignment 12\\Programming\\modules.py\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    return DY\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import modules,utils\n",
    "%matplotlib inline\n",
    "\n",
    "nn  = modules.Sequential([\n",
    "    modules.Linear(2,20,seed=0),\n",
    "    modules.Identity(),\n",
    "])\n",
    "svc = utils.LinearSVC(C=1.0)\n",
    "\n",
    "train(nn,svc,msteps=[1,10,100],vsteps=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Adding Nonlinearity to the Feature Map (25 P)\n",
    "\n",
    "In order to learn a nonlinear decision boundary that fits the dataset, we need to introduce nonlinearity into the feature map.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "*  **Write a class `Sigmoid` that inherits `modules.Module` and implements a layer of sigmoid nonlinearity (as defined in the slides).**\n",
    "*  **Keeping the dimensionality of the feature map to 20, replace the identity activation function of the neural network above by the sigmoid activation function that you have implemented.**\n",
    "*  **Run the code, and compare the value of the SVM objective to the one obtained with the linear network.**\n",
    "*  **Give an interpretation of the resulting plots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import solutions\n",
    "%matplotlib inline\n",
    "solutions.A(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Adding More Layers (25 P)\n",
    "\n",
    "One layer of nonlinearity might be enough to separate the data, but might not be sufficient in order to support a large-margin classifier. In this case, one can add a additional layer of nonlinearity to the neural network in order to increase its representational power.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* **Construct a network with *two* layers of sigmoid nonlinearity. The first layer of neurons has dimension 10 and the second layer has dimension 20.**\n",
    "* **Run the code, and compare the value of the SVM objective to the one obtained when using only one layer of nonlinearity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import solutions\n",
    "%matplotlib inline\n",
    "solutions.B1(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "\n",
    "* **Repeat the experiment for a network with *three* layers of sigmoid nonlinearity. The first layer of neurons has dimension 5, the second layer has dimension 10 and the third layer has dimension 20.**\n",
    "* **Run the code, and compare the value of the SVM objective to the one obtained when using only one or two layers of nonlinearity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import solutions\n",
    "%matplotlib inline\n",
    "solutions.B2(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
